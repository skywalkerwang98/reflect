{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Register for an [OpenAI API key](https://openai.com/blog/openai-api/) to use GPT-4 (there's a free trial) and enter it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "API_BASE_URL = os.getenv('OPENAI_API_BASE_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add HuggingFace mirror endpoint to avoid connection timeout issues when downloading models\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# Add proxy settings to avoid connection issues\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:15732\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:15732\" \n",
    "os.environ[\"all_proxy\"] = \"socks5://127.0.0.1:15732\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd main\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from main.gen_data import *\n",
    "from main.data import load_data\n",
    "from main.exp import *\n",
    "from main.execute_replan import run_correction\n",
    "from LLM.prompt import LLMPrompter\n",
    "\n",
    "# You may change the GPT version here\n",
    "llm_prompter = LLMPrompter(gpt_version=\"gpt-4o\", api_key=API_KEY, base_url=API_BASE_URL)\n",
    "\n",
    "with open('tasks.json') as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "def show_video(video_path, video_width=300):\n",
    "  video_file = open(video_path, \"r+b\").read()\n",
    "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation in Simulation\n",
    "We provide a few example task configurations in ``tasks.json``. Let's take the first one as an example.\n",
    "\n",
    "The robot task is to \"make coffee\" and the robot failed because it cannot \"put the mug inside the coffee machine because there was already a cup inside it, occupying the space\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info = {\n",
    "    \"name\": \"make coffee\", # name of the task\n",
    "    \"task_idx\": 5, # index of the task, defined in TASK_LIST in constants.py\n",
    "    \"num_samples\": 1, # the number of samples to generate, this applies for randomly injected failures (e.g. see \"Task 6\" in tasks.json, to automatically generate two dropping failures which occurred at different times)\n",
    "    \"failure_injection\": False, # whether to inject failures manaully or automatically\n",
    "    \"folder_name\": \"makeCoffee-1\", # name of the folder to save data\n",
    "    \"scene\": \"FloorPlan16\", # scene id as in ai2thor\n",
    "    \"chosen_failure\": \"occupied\", # selected failure type, can also be blocking, occupied_put, ambiguous_plan, wrong_perception, drop, failed_action, and missing_step. See tasks.json for examples.\n",
    "    \"gt_failure_reason\": \"The robot failed to put the mug inside the coffee machine because there was already a cup inside it, occupying the space.\",\n",
    "    \"gt_failure_step\": \"00:51\",\n",
    "    \"preactions\": [ # actions taken to set object state before task execution\n",
    "        \"(dirty_obj, Mug)\"\n",
    "    ],\n",
    "    \"failure_injection_params\" : { # parameters used to configure the environment according to chosen_failure\n",
    "        \"src_obj_type\": \"Cup\",\n",
    "        \"target_obj_type\": \"CoffeeMachine\",\n",
    "        \"disp_x\": 0.0,\n",
    "        \"disp_z\": 0.05,\n",
    "        \"disp_y\": 0.02\n",
    "    },\n",
    "    \"actions\": [ # the original robot plan\n",
    "        \"(navigate_to_obj, Mug)\",\n",
    "        \"(pick_up, Mug)\",\n",
    "        \"(navigate_to_obj, Sink)\",\n",
    "        \"(put_on, Mug, SinkBasin)\",\n",
    "        \"(toggle_on, Faucet)\",\n",
    "        \"(toggle_off, Faucet)\",\n",
    "        \"(pick_up, Mug)\",\n",
    "        \"(pour, Mug, Sink)\",\n",
    "        \"(navigate_to_obj, CoffeeMachine)\",\n",
    "        \"(put_in, Mug, CoffeeMachine)\",\n",
    "        \"(toggle_on, CoffeeMachine)\",\n",
    "        \"(toggle_off, CoffeeMachine)\",\n",
    "        \"(pick_up, Mug)\",\n",
    "        \"(put_on, Mug, CounterTop)\"\n",
    "    ],\n",
    "    \"success_condition\": \"a clean mug is filled with coffee and on top of the countertop.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data will be saved under {data_path}/thor_tasks/makeCoffee/makeCoffee-1.\n",
    "run_data_gen(data_path=os.getcwd(), task=task_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'makeCoffee/makeCoffee-1'  # specify the task folder name here. In this example, it's makeCoffee/makeCoffee-1.\n",
    "show_video(f'thor_tasks/{FOLDER_NAME}/original-video.mp4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Summary\n",
    "Once we have the task execution data, we can generate a hierarchical summary of the robot experiences. The summary contains 3 levels:\n",
    "1. The sensory input summary will convert raw sensory data (RGB-D, audio, robot state) into a structured format. \n",
    "2. The event-based summary is composed of captions for selected key event frames (e.g. changes in visual scene graph, audio event, robot event). \n",
    "3. The subgoal-based summary is composed of end frame of each subgoal to facilitate faster failure localization.\n",
    "\n",
    "Please check `main/state_summary/makeCoffee/makeCoffee-1` for the generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_AUDIO = 1 # 1: using audio deteceted with wav2clip, 0: using ground truth audio information\n",
    "events, task, object_list, interact_actions, nav_actions = load_data(f\"thor_tasks/{FOLDER_NAME}\", task_info)\n",
    "print(len(events))\n",
    "\n",
    "# Sensory-input summary\n",
    "detected_sounds = []\n",
    "if WITH_AUDIO == 1:\n",
    "    detected_sounds = run_sound_module(FOLDER_NAME, object_list)\n",
    "generate_scene_graphs(FOLDER_NAME, events, object_list, nav_actions, interact_actions, WITH_AUDIO, detected_sounds)\n",
    "with open(f'state_summary/{FOLDER_NAME}/global_sg.pkl', 'rb') as f:\n",
    "    global_sg = pickle.load(f)\n",
    "    print(\"================ Global SG ================\")\n",
    "    print(global_sg)\n",
    "\n",
    "# Event-based summary & Subgoal-based summary\n",
    "generate_summary(FOLDER_NAME, events, nav_actions, interact_actions, WITH_AUDIO, detected_sounds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Reasoning and Correction with LLM\n",
    "`LLM/prompts.json` contains all prompt template used to query GPT-4.\n",
    "\n",
    "The failure explanation and correction plan generated by LLM will automatically be saved under `LLM/makeCoffee/makeCoffee-1/response.json`. You can disable auto-save by changing the \"save\" parameter in `LLM/prompts.json` to \"false\" for these two queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reasoning(FOLDER_NAME, llm_prompter, global_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_replan(FOLDER_NAME, llm_prompter, global_sg, events[-1], object_list)\n",
    "run_correction(data_path=os.getcwd(), f_name=FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(f'recovery/{FOLDER_NAME}/recovery-video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example\n",
    "Let's see another example. The robot task is to \"boil water\" and the robot failed because \"it missed the step to pick up the pot from sink before moving to stove burner\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info = tasks[\"Task 2\"]\n",
    "task_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_gen(data_path=os.getcwd(), task=task_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'boilWater/boilWater-1'\n",
    "show_video(f'thor_tasks/{FOLDER_NAME}/original-video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_AUDIO = 1\n",
    "events, task, object_list, interact_actions, nav_actions = load_data(f\"thor_tasks/{FOLDER_NAME}\", task_info)\n",
    "print(len(events))\n",
    "\n",
    "# ===========sensory-input summary============\n",
    "detected_sounds = []\n",
    "if WITH_AUDIO == 1:\n",
    "    detected_sounds = run_sound_module(FOLDER_NAME, object_list)\n",
    "generate_scene_graphs(FOLDER_NAME, events, object_list, nav_actions, interact_actions, WITH_AUDIO, detected_sounds)\n",
    "with open(f'state_summary/{FOLDER_NAME}/global_sg.pkl', 'rb') as f:\n",
    "    global_sg = pickle.load(f)\n",
    "    print(\"================ Global SG ================\")\n",
    "    print(global_sg)\n",
    "\n",
    "# ===========event-based summary & subgoal-based summary============\n",
    "generate_summary(FOLDER_NAME, events, nav_actions, interact_actions, WITH_AUDIO, detected_sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reasoning(FOLDER_NAME, llm_prompter, global_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_replan(FOLDER_NAME, llm_prompter, global_sg, events[-1], object_list)\n",
    "run_correction(data_path=os.getcwd(), f_name=FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(f'recovery/{FOLDER_NAME}/recovery-video.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflect_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
